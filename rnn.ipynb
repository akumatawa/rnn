{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":6868,"status":"ok","timestamp":1657691743994,"user":{"displayName":"Никита Ахматов","userId":"01614573069225518564"},"user_tz":-240},"id":"tZWeLDofAil5"},"outputs":[],"source":["import os\n","import re\n","from tqdm import tqdm\n","import numpy as np\n","from keras.preprocessing import sequence\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Flatten, SimpleRNN\n","from keras.layers import Conv1D, GlobalMaxPooling1D\n","from nltk.tokenize import TreebankWordTokenizer\n","from random import shuffle"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1657691743995,"user":{"displayName":"Никита Ахматов","userId":"01614573069225518564"},"user_tz":-240},"id":"642VtJJeHHKG"},"outputs":[],"source":["train_filepath = 'aclImdb/train/'\n","test_filepath = 'aclImdb/test/'"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1657691743996,"user":{"displayName":"Никита Ахматов","userId":"01614573069225518564"},"user_tz":-240},"id":"MlABRxYT-P2w"},"outputs":[],"source":["# # Vocabulary: All words used, starting by the most frequent\n","# with open('aclImdb/imdb.vocab') as f:\n","#     vocab = [word.rstrip() for word in f]\n","#     # Keep only most frequent 5000 words rather than all 90000\n","#     # Just saving memory - the long tail occurs too few times\n","#     # for the model to learn anything anyway\n","#     vocab = vocab[:5000]\n","#     print('%d words in vocabulary' % (len(vocab),))"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1657691743996,"user":{"displayName":"Никита Ахматов","userId":"01614573069225518564"},"user_tz":-240},"id":"AYFEKFd_VXHQ"},"outputs":[],"source":["# files = []\n","# dirs = []\n","# for (dir_path, dir_names, file_names) in os.walk('aclImdb'):\n","#     dirs.extend(dir_names)\n","#     files.extend(file_names)\n","# print(dirs)\n","# print(len(files))"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1657691743997,"user":{"displayName":"Никита Ахматов","userId":"01614573069225518564"},"user_tz":-240},"id":"Hlb_Yp1ZXK5A"},"outputs":[],"source":["# dir_path = 'aclImdb/train/pos'\n","\n","# files = []\n","# folders = []\n","\n","# # Iterate directory\n","# for path in os.listdir(dir_path):\n","#     # check if current path is a file\n","#     if os.path.isfile(os.path.join(dir_path, path)):\n","#         files.append(path)\n","#     else:\n","#         folders.append(path)\n","# print(files)\n","# print(folders)"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1657691743997,"user":{"displayName":"Никита Ахматов","userId":"01614573069225518564"},"user_tz":-240},"id":"DBrQilZnD_II"},"outputs":[],"source":["# !    wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n","# !    tar xfz aclImdb_v1.tar.gz"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":20315,"status":"ok","timestamp":1657691764302,"user":{"displayName":"Никита Ахматов","userId":"01614573069225518564"},"user_tz":-240},"id":"5FweFTuF_bIA"},"outputs":[],"source":["def pre_process_data(filepath):\n","    paths = ['aclImdb/train/', 'aclImdb/test/']\n","    # positive_path = 'aclImdb/train/pos/'\n","    # negative_path = 'aclImdb/train/neg/'\n","    pos_label = 1\n","    neg_label = 0\n","    dataset = []\n","\n","    for path in paths:\n","        positive_path = path + 'pos/'\n","        negative_path = path + 'neg/'\n","\n","        pos_files = []\n","        # Iterate directory\n","        for path in os.listdir(positive_path):\n","            # check if current path is a file\n","            if os.path.isfile(os.path.join(positive_path, path)):\n","                pos_files.append(path)\n","\n","        for filename in pos_files:\n","            with open(positive_path + filename, 'r') as f:\n","                dataset.append((pos_label, f.read()))\n","\n","        \n","        neg_files = []\n","        # Iterate directory\n","        for path in os.listdir(negative_path):\n","            # check if current path is a file\n","            if os.path.isfile(os.path.join(negative_path, path)):\n","                neg_files.append(path)\n","\n","        for filename in neg_files:\n","            with open(negative_path + filename, 'r') as f:\n","                dataset.append((neg_label, f.read()))\n","    \n","    shuffle(dataset)\n","\n","    return dataset\n","\n","dataset = pre_process_data(train_filepath)"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":371379,"status":"ok","timestamp":1657692135660,"user":{"displayName":"Никита Ахматов","userId":"01614573069225518564"},"user_tz":-240},"id":"p6ZUh50bNtQU","outputId":"fe44ee08-f42c-4396-942f-a538ee082e7c"},"outputs":[{"name":"stdout","output_type":"stream","text":["--2022-07-13 05:56:03--  https://nlp.stanford.edu/data/glove.6B.zip\n","Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n","Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n","--2022-07-13 05:56:04--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n","Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n","Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 862182613 (822M) [application/zip]\n","Saving to: ‘glove.6B.zip.1’\n","\n","glove.6B.zip.1      100%[===================\u003e] 822.24M  4.98MB/s    in 2m 42s  \n","\n","2022-07-13 05:58:46 (5.08 MB/s) - ‘glove.6B.zip.1’ saved [862182613/862182613]\n","\n","Archive:  glove.6B.zip\n","replace glove.6B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n","replace glove.6B.100d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n","replace glove.6B.200d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n","replace glove.6B.300d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n"]}],"source":["# ! wget https://nlp.stanford.edu/data/glove.6B.zip\n","# ! unzip glove.6B.zip"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5033,"status":"ok","timestamp":1657692140633,"user":{"displayName":"Никита Ахматов","userId":"01614573069225518564"},"user_tz":-240},"id":"mP-aclKQN43-","outputId":"8026b0e2-d959-4609-d4e8-fc79d5df709e"},"outputs":[{"name":"stderr","output_type":"stream","text":["400000it [00:05, 77230.89it/s]"]},{"name":"stdout","output_type":"stream","text":["Found 400000 word vectors.\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# let's create a dictionary of each word in the pre-trained GloVe embeddings, saving its location indexes \n","EMBEDDING_DIM = 50\n","\n","GLOVE_DIR = \".\"\n","word_vectors = {}\n","f = open(os.path.join(GLOVE_DIR, 'glove.6B.%dd.txt' % EMBEDDING_DIM))\n","for line in tqdm(f):\n","    values = line.split()\n","    word = values[0]\n","    coefs = np.asarray(values[1:], dtype='float32')\n","    word_vectors[word] = coefs\n","f.close()\n","\n","print('Found %s word vectors.' % len(word_vectors))"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1657692140635,"user":{"displayName":"Никита Ахматов","userId":"01614573069225518564"},"user_tz":-240},"id":"HePHbkeKA3u5"},"outputs":[],"source":["def tokenize_and_vectorize(dataset):\n","    tokenizer = TreebankWordTokenizer()\n","    vectorized_data = []\n","    expected = []\n","    for sample in dataset:\n","        tokens = tokenizer.tokenize(sample[1])\n","        sample_vecs = []\n","        for token in tokens:\n","            try:\n","                sample_vecs.append(word_vectors[token])\n","            \n","            except KeyError:\n","                pass # No matching token in w2v vocab\n","        \n","        vectorized_data.append(sample_vecs)\n","\n","    return vectorized_data"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1657692140636,"user":{"displayName":"Никита Ахматов","userId":"01614573069225518564"},"user_tz":-240},"id":"Ggz5qxHyJHNY"},"outputs":[],"source":["def collect_expected(dataset):\n","    expected = []\n","    for sample in dataset:\n","        expected.append(sample[0])\n","    \n","    return expected"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":40422,"status":"ok","timestamp":1657692181046,"user":{"displayName":"Никита Ахматов","userId":"01614573069225518564"},"user_tz":-240},"id":"HUUXtO8wJWbb"},"outputs":[],"source":["vectorized_data = tokenize_and_vectorize(dataset)\n","expected = collect_expected(dataset)"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":35,"status":"ok","timestamp":1657692181047,"user":{"displayName":"Никита Ахматов","userId":"01614573069225518564"},"user_tz":-240},"id":"1CtqtUejQCiV"},"outputs":[],"source":["split_point_train = int(len(vectorized_data)*.6)\n","split_point_val = int(len(vectorized_data)*.8)\n","\n","x_train = vectorized_data[:split_point_train]\n","y_train = expected[:split_point_train]\n","\n","x_val = vectorized_data[split_point_train:split_point_val]\n","y_val = expected[split_point_train:split_point_val]\n","\n","x_test = vectorized_data[split_point_val:]\n","y_test = expected[split_point_val:]"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":35,"status":"ok","timestamp":1657692181048,"user":{"displayName":"Никита Ахматов","userId":"01614573069225518564"},"user_tz":-240},"id":"EvNMrLQACasq"},"outputs":[],"source":["maxlen = 400\n","batch_size = 8\n","embedding_dims = 50\n","epochs = 20"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":35,"status":"ok","timestamp":1657692181049,"user":{"displayName":"Никита Ахматов","userId":"01614573069225518564"},"user_tz":-240},"id":"7HSMRwhbFFHB"},"outputs":[],"source":["def pad_trunc(data, maxlen):\n","    new_data = []\n","\n","    # Create a vector of 0s the length of our word vector\n","    zero_vector = []\n","    for _ in range(len(data[0][0])):\n","        zero_vector.append(0.0)\n","\n","    for sample in data:\n","        if len(sample) \u003e maxlen:\n","            temp = sample[:maxlen]\n","        elif len(sample) \u003c maxlen:\n","            temp = sample\n","\n","            # Append the appropriate number 0 vectors to the last\n","            additional_elems = maxlen - len(sample)\n","            for _ in range (additional_elems):\n","                temp.append(zero_vector)\n","        else:\n","            temp = sample\n","        new_data.append(temp)\n","    return new_data"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":33745,"status":"ok","timestamp":1657692214760,"user":{"displayName":"Никита Ахматов","userId":"01614573069225518564"},"user_tz":-240},"id":"JDvQfwf-R-D7"},"outputs":[],"source":["x_train = pad_trunc(x_train, maxlen)\n","x_val = pad_trunc(x_val, maxlen)\n","x_test = pad_trunc(x_test, maxlen)\n","\n","x_train = np.reshape(x_train, (len(x_train), maxlen, embedding_dims))\n","y_train = np.array(y_train)\n","\n","x_val = np.reshape(x_val, (len(x_val), maxlen, embedding_dims))\n","y_val = np.array(y_val)"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1657692214761,"user":{"displayName":"Никита Ахматов","userId":"01614573069225518564"},"user_tz":-240},"id":"IQPV1YG-Ciaj","outputId":"e3d9f9d6-55df-4703-8eac-2b4185897fd3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," simple_rnn (SimpleRNN)      (None, 400, 25)           1900      \n","                                                                 \n"," dropout (Dropout)           (None, 400, 25)           0         \n","                                                                 \n"," flatten (Flatten)           (None, 10000)             0         \n","                                                                 \n"," dense (Dense)               (None, 1)                 10001     \n","                                                                 \n","=================================================================\n","Total params: 11,901\n","Trainable params: 11,901\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["num_neurons = 25\n","model = Sequential()\n","model.add(SimpleRNN(\n","    num_neurons, return_sequences=True,\n","    input_shape=(maxlen, embedding_dims)))\n","model.add(Dropout(.2))\n","model.add(Flatten())\n","model.add(Dense(1, activation='sigmoid'))\n","model.compile('rmsprop', 'binary_crossentropy', metrics=['accuracy'])\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"MtBMiUFwDA-O"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","3750/3750 [==============================] - 200s 53ms/step - loss: 0.7151 - accuracy: 0.6447 - val_loss: 0.6230 - val_accuracy: 0.6998\n","Epoch 2/20\n","3750/3750 [==============================] - 198s 53ms/step - loss: 0.6152 - accuracy: 0.7126 - val_loss: 0.6634 - val_accuracy: 0.6949\n","Epoch 3/20\n","3750/3750 [==============================] - 199s 53ms/step - loss: 0.5735 - accuracy: 0.7345 - val_loss: 0.6324 - val_accuracy: 0.7053\n","Epoch 4/20\n","3750/3750 [==============================] - 199s 53ms/step - loss: 0.5553 - accuracy: 0.7428 - val_loss: 0.6412 - val_accuracy: 0.7129\n","Epoch 5/20\n","3750/3750 [==============================] - 199s 53ms/step - loss: 0.5333 - accuracy: 0.7544 - val_loss: 0.6918 - val_accuracy: 0.7016\n","Epoch 6/20\n","3750/3750 [==============================] - 197s 52ms/step - loss: 0.5212 - accuracy: 0.7610 - val_loss: 0.6544 - val_accuracy: 0.7149\n","Epoch 7/20\n","3750/3750 [==============================] - 198s 53ms/step - loss: 0.5150 - accuracy: 0.7656 - val_loss: 0.6573 - val_accuracy: 0.7151\n","Epoch 8/20\n","3750/3750 [==============================] - 201s 54ms/step - loss: 0.5090 - accuracy: 0.7712 - val_loss: 0.6550 - val_accuracy: 0.7175\n","Epoch 9/20\n","3750/3750 [==============================] - 202s 54ms/step - loss: 0.5033 - accuracy: 0.7737 - val_loss: 0.6689 - val_accuracy: 0.7217\n","Epoch 10/20\n","3750/3750 [==============================] - 202s 54ms/step - loss: 0.4965 - accuracy: 0.7774 - val_loss: 0.7512 - val_accuracy: 0.6886\n","Epoch 11/20\n","3750/3750 [==============================] - 203s 54ms/step - loss: 0.4881 - accuracy: 0.7821 - val_loss: 0.6674 - val_accuracy: 0.7252\n","Epoch 12/20\n","3750/3750 [==============================] - 203s 54ms/step - loss: 0.4853 - accuracy: 0.7844 - val_loss: 0.7961 - val_accuracy: 0.6968\n","Epoch 13/20\n","3750/3750 [==============================] - 203s 54ms/step - loss: 0.4804 - accuracy: 0.7879 - val_loss: 0.7745 - val_accuracy: 0.6942\n","Epoch 14/20\n","3750/3750 [==============================] - 198s 53ms/step - loss: 0.4819 - accuracy: 0.7859 - val_loss: 0.7168 - val_accuracy: 0.7219\n","Epoch 15/20\n","3750/3750 [==============================] - 201s 54ms/step - loss: 0.4792 - accuracy: 0.7883 - val_loss: 0.7533 - val_accuracy: 0.7028\n","Epoch 16/20\n","3750/3750 [==============================] - 200s 53ms/step - loss: 0.4726 - accuracy: 0.7904 - val_loss: 0.7250 - val_accuracy: 0.7210\n","Epoch 17/20\n","3750/3750 [==============================] - 200s 53ms/step - loss: 0.4825 - accuracy: 0.7899 - val_loss: 0.7368 - val_accuracy: 0.7174\n","Epoch 18/20\n","3750/3750 [==============================] - 201s 54ms/step - loss: 0.4709 - accuracy: 0.7948 - val_loss: 0.7256 - val_accuracy: 0.7263\n","Epoch 19/20\n","3750/3750 [==============================] - 203s 54ms/step - loss: 0.4726 - accuracy: 0.7948 - val_loss: 0.7297 - val_accuracy: 0.7234\n","Epoch 20/20\n","3750/3750 [==============================] - 202s 54ms/step - loss: 0.4643 - accuracy: 0.8004 - val_loss: 0.7506 - val_accuracy: 0.7153\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7fa13372e850\u003e"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["model.fit(x_train, y_train,\n","    batch_size=batch_size,\n","    epochs=epochs,\n","    validation_data=(x_val, y_val))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"b_il2jkzB-AH"},"outputs":[],"source":["x_train = None\n","y_train = None\n","\n","x_val = None\n","y_val = None"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"wgtdWm1NHSd9"},"outputs":[],"source":["x_test = np.reshape(x_test, (len(x_test), maxlen, embedding_dims))\n","y_test = np.array(y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CD-g9HAk97JX"},"outputs":[],"source":["# Evaluate the model on the test data\n","print(\"Evaluate on test data\")\n","predictions = model.predict(x_test)\n","\n","y_test = np.reshape(y_test, (len(predictions), 1))\n","\n","from google.colab import drive\n","drive.mount('/drive')\n","\n","model_structure = model.to_json()\n","filename = f\"/drive/My Drive/science/ml={maxlen}__n={num_neurons}.npy\"\n","with open(filename, \"wb\") as f:\n","    np.save(f, np.array([predictions, y_test]))\n","            \n","drive.flush_and_unmount()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6F8A9KsNSizb"},"outputs":[],"source":["drive.mount('/drive')\n","\n","model_structure = model.to_json()\n","filename = f\"/drive/My Drive/science/rnn_model_ml={maxlen}__n={num_neurons}.json\"\n","\n","with open(filename, \"w\") as json_file:\n","    json_file.write(model_structure)\n","    \n","model.save_weights(\"cnn_weights.h5\")\n","drive.flush_and_unmount()"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOBoinRMdB0x681nqpVR1Yp","collapsed_sections":[],"name":"rnn.ipynb","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}